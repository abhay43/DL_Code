{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Overview of Colaboratory Features",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhay43/ML_Code/blob/master/RL_Block_Problem_AS12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5cYsOeSVRSo",
        "colab_type": "text"
      },
      "source": [
        "The mouse can take actions in each direction  (UP=0, RIGHT=1, DOWN=2, \n",
        "LEFT=3). \n",
        "\n",
        "Model of the maze environment, wherein the states are defined is provided for \n",
        "\n",
        "your reference.  Any action that takes the mouse beyond the maze will result in \n",
        "\n",
        "the mouse staying in the same state.  Mouse receives a reward of -1 at each \n",
        "\n",
        "step until it reaches the cheese block.\n",
        "\n",
        "Carry out the following tasks:\n",
        "\n",
        "1.  Find an optimal  policy through Value  Iteration method that takes the \n",
        "         \n",
        "       mouse to the cheese block starting from any internal state.\n",
        "\n",
        "2.  Print the optimal policy to reach the cheese block.\n",
        "\n",
        "3.   Print your output to a csv file.\n",
        "\n",
        " Output Format:\n",
        "   \n",
        " Write the output  received towards the end in output.csv, which should be\n",
        "\n",
        " present at the location /code/output/output.csv\n",
        "\n",
        "Example:  output.csv will have data that looks as given below: \n",
        "\n",
        "Given below is the sample output just for reference ï»¿\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvXB4kUBk5Gr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "import pandas as pd\n",
        "\n",
        "ACTION_UP    =  0\n",
        "ACTION_RIGHT =  1\n",
        "ACTION_DOWN  =  2\n",
        "ACTION_LEFT  =  3\n",
        "\n",
        "class GridWorld(object):\n",
        "    def __init__(self,shape=[5,5]):\n",
        "        self.shape = shape\n",
        "        numStates  = shape[0] * shape[1]\n",
        "        numActions = 4\n",
        "        self.numStates = numStates\n",
        "        self.numActions = numActions        \n",
        "        xmax = shape[0]\n",
        "        ymax = shape[1]\n",
        "\n",
        "        grid = np.arange(numStates).reshape(shape)\n",
        "        Model = {}\n",
        "\n",
        "        x_indices = np.arange(xmax)\n",
        "        y_indices = np.arange(ymax)\n",
        "\n",
        "        for x in x_indices:\n",
        "            for y in y_indices:\n",
        "                state = y + x*(xmax)\n",
        "                #print(x,y,state)\n",
        "                Model[state] ={action:[] for action in np.arange(numActions)}\n",
        "\n",
        "                is_terminal_state = lambda state : state == 0 or state == (numStates-1)\n",
        "                reward = 0.0 if is_terminal_state(state) else -1.0\n",
        "\n",
        "\n",
        "                if is_terminal_state(state):\n",
        "                    Model[state][ACTION_UP] = [(1.0,state,reward,True)]\n",
        "                    Model[state][ACTION_RIGHT] = [(1.0,state,reward,True)]\n",
        "                    Model[state][ACTION_DOWN] = [(1.0,state,reward,True)]\n",
        "                    Model[state][ACTION_LEFT] = [(1.0,state,reward,True)]\n",
        "                else:\n",
        "                    next_state = {}\n",
        "                    next_state[ACTION_UP] = state if x == 0 else state - ymax\n",
        "                    next_state[ACTION_RIGHT] = state if y == ymax-1 else state +1\n",
        "                    next_state[ACTION_DOWN] = state if x == xmax-1 else state + ymax\n",
        "                    next_state[ACTION_LEFT] = state if y == 0 else state -1 \n",
        "                    Model[state][ACTION_UP] = [(1.0,next_state[ACTION_UP] ,reward,is_terminal_state(next_state[ACTION_UP]))]\n",
        "                    Model[state][ACTION_RIGHT] = [(1.0,next_state[ACTION_RIGHT],reward,is_terminal_state(next_state[ACTION_RIGHT]))]\n",
        "                    Model[state][ACTION_DOWN] = [(1.0,next_state[ACTION_DOWN],reward,is_terminal_state(next_state[ACTION_DOWN]))]\n",
        "                    Model[state][ACTION_LEFT] = [(1.0,next_state[ACTION_LEFT],reward,is_terminal_state(next_state[ACTION_LEFT]))]\n",
        "        self.model = Model\n",
        "\n",
        "# Write a function to calculate the value for all the actions in a given state  \n",
        "\n",
        "# def value_iteration(env, theta=0.0001, discount_factor=1.0):   \n",
        " #  print('test')\n",
        "\n",
        "    # Write your code here\n",
        "    \n",
        "# Write the code to create a deterministic policy by using the optimal value function. Print the optimal policy.\n",
        "\n",
        "     # Write your code here\n",
        "    \n",
        "# Save the final output as described in the sample format given below \n",
        "\n",
        "# data= np.array([[1, 2, 3, 4, 5], [0, 0, 4, 1, 2],[3, 4, 1, 0, 2],[2, 1, 3, 4, 5],[2, 1, 3, 4, 5]])\n",
        "# output=pd.DataFrame(data)\n",
        "# output.to_csv('/code/output/output.csv', header=False, index=False) \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2B5lDOU-gvE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def value_eveluation(policy, env, discount_factor=1.0, theta=0.00001):\n",
        "  value_f = np.zeros(env.numStates)\n",
        "  while True:\n",
        "    delta = 0\n",
        "    for state in range(env.numStates):\n",
        "      state_value =0\n",
        "      for action,action_prob in enumerate(policy[state]):\n",
        "        for  prob, next_state, reward, done in env.model[state][action]:\n",
        "          state_value += action_prob * prob * (reward + discount_factor * value_f[next_state])\n",
        "        delta = max(delta, np.abs(state_value - value_f[state]))\n",
        "        value_f[state] = state_value\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return value_f\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypzrrvxqH6b3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = GridWorld()\n",
        "def value_iteration(env, policy_eval_fn=value_eveluation, discount_factor=1.0):\n",
        "  \n",
        "  def compute_value_fn_update(state,value_fn):\n",
        "    value_fn_update = np.zeros(env.numActions)\n",
        "    for action in range(env.numActions):\n",
        "        for prob,next_state,reward,done in env.model[state][action]:\n",
        "            value_fn_update[action] += prob * (reward + discount_factor * value_fn[next_state])                \n",
        "    return value_fn_update \n",
        "\n",
        "  while True:    \n",
        "    policy = np.ones([env.numStates,env.numActions]) /env.numActions\n",
        "    value_f = value_eveluation(policy, env, discount_factor)\n",
        "    policy_stable = True\n",
        "    # improve the policy\n",
        "    for state in range(env.numStates):\n",
        "      select_action = np.argmax(policy[state])\n",
        "      valueFun = compute_value_fn_update(state,value_f )\n",
        "      best_action = np.argmax(valueFun)\n",
        "      if select_action != best_action:\n",
        "        policy_stable = False\n",
        "      policy[state] = np.eye(env.numActions)[best_action]\n",
        "      if policy_stable:\n",
        "        return policy, value_f\n",
        "\n",
        "  while True:\n",
        "    delta = 0\n",
        "    for state in range(env.numStates):\n",
        "      state_value = 0\n",
        "      for action,action_prob in enumerate(policy[state]):\n",
        "              for  prob, next_state, reward, done in env.model[state][action]:\n",
        "                  state_value += action_prob * prob * (reward + discount_factor * value_f[next_state])\n",
        "              delta = max(delta, np.abs(state_value - value_f[state]))\n",
        "              value_f[state] = state_value\n",
        "      # Stop evaluating once our value function change is below a threshold\n",
        "      if delta < theta:\n",
        "          break\n",
        "  return value_f\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikghzfjmnqCq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "policy, v = value_iteration(env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Xeesn6Rn7Ms",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "aa05f149-af83-476c-9b2f-691822a271b6"
      },
      "source": [
        "v.reshape(5,5)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.        , -1.        , -1.25      , -1.3125    , -1.390625  ],\n",
              "       [-1.1875    , -1.546875  , -1.69921875, -1.75292969, -1.93530273],\n",
              "       [-1.55859375, -1.77636719, -1.86889648, -1.90545654, -2.14364624],\n",
              "       [-1.67456055, -1.86273193, -1.9329071 , -1.95959091, -2.22228718],\n",
              "       [-1.99787521, -2.20657253, -2.2806766 , -2.30754131,  0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL39l3x4SP8I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output=pd.DataFrame(v.reshape(5,5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tPTN_94S8j9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "1e70a5a8-5026-40e3-e21e-7761b1618aa7"
      },
      "source": [
        "output.head(100)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.250000</td>\n",
              "      <td>-1.312500</td>\n",
              "      <td>-1.390625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.187500</td>\n",
              "      <td>-1.546875</td>\n",
              "      <td>-1.699219</td>\n",
              "      <td>-1.752930</td>\n",
              "      <td>-1.935303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.558594</td>\n",
              "      <td>-1.776367</td>\n",
              "      <td>-1.868896</td>\n",
              "      <td>-1.905457</td>\n",
              "      <td>-2.143646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.674561</td>\n",
              "      <td>-1.862732</td>\n",
              "      <td>-1.932907</td>\n",
              "      <td>-1.959591</td>\n",
              "      <td>-2.222287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.997875</td>\n",
              "      <td>-2.206573</td>\n",
              "      <td>-2.280677</td>\n",
              "      <td>-2.307541</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2         3         4\n",
              "0  0.000000 -1.000000 -1.250000 -1.312500 -1.390625\n",
              "1 -1.187500 -1.546875 -1.699219 -1.752930 -1.935303\n",
              "2 -1.558594 -1.776367 -1.868896 -1.905457 -2.143646\n",
              "3 -1.674561 -1.862732 -1.932907 -1.959591 -2.222287\n",
              "4 -1.997875 -2.206573 -2.280677 -2.307541  0.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBjHV1iaS-5K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}